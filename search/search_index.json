{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Features \u23f0 Task scheduling <p>Based on APScheduler, it supports Interval, Cron and Date triggers</p> \ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc0d Python pipelines <p>Define pipelines and tasks in pure Python: if you can write a function, you can write a pipeline</p> \ud83c\udf9b\ufe0f Parametrized pipelines <p>Use Pydantic to define parameters,        and get a nice web form to run your pipelines</p> \ud83d\udc49 Manual runs <p>Run pipelines manually from the web UI, just click the button</p> \ud83d\udcbb Built-in Web interface <p>No HTML/JS/CSS coding required</p> \ud83d\udd0d Debugging <p>Explore logs and output data</p> \ud83d\udd10 Secured <p>Optional OAuth2 authentication</p> \ud83d\udca3 REST API <p>For advanced integrations use the REST API</p> POST https://plombery.com/api/pipelines/run \ud83d\udce9 Monitoring <p>Get alerted if something goes wrong</p>"},{"location":"create-a-pipeline/","title":"Create your first pipeline","text":"<p>Create a new folder in your project root with a file named <code>app.py</code> (or any name you want) in it, as in Python files should be in a top-level package.</p> <p>This should be your folder structure:</p> <pre><code>.\n\u251c\u2500 .venv/ # virtual environment folder\n\u2514\u2500 src/\n   \u251c\u2500 __init__.py # empty file needed to declare Python modules\n   \u2514\u2500 app.py # entrypoint to the project\n</code></pre>"},{"location":"create-a-pipeline/#glossary","title":"Glossary","text":"<p>Before starting, let's define some naming so there will be no confusion!</p> <ul> <li>Task: a python function that performs some job, it's the base block for building a pipeline</li> <li>Pipeline: a sequence of 1 or more Tasks, a pipeline can be run via a schedule, manually, etc.</li> <li>Trigger: is the entrypoint to run a pipeline, a trigger can be a schedule, a webhook, a button on the web UI, etc.</li> <li>Pipeline Run: (sometimes simply referred as Run) is the result of running a pipeline</li> </ul>"},{"location":"create-a-pipeline/#basic-pipeline","title":"Basic pipeline","text":""},{"location":"create-a-pipeline/#create-a-task","title":"Create a task","text":"<p>A Task is the base block in Plombery and it's just a Python function that performs an action, i.e. download some data from an HTTP API, runs a query on a DB, etc.</p> <p>Info</p> <p>notice how the <code>@task</code> decorator is used to declare a task</p> src/app.py<pre><code>from datetime import datetime\nfrom random import randint\n\nfrom apscheduler.triggers.interval import IntervalTrigger\nfrom plombery import task, get_logger, Trigger, register_pipeline\n\n\n@task\nasync def fetch_raw_sales_data():\n    \"\"\"Fetch latest 50 sales of the day\"\"\"\n\n    # using Plombery logger your logs will be stored\n    # and accessible on the web UI\n    logger = get_logger()\n\n    logger.debug(\"Fetching sales data...\")\n\n    sales = [\n        {\n            \"price\": randint(1, 1000),\n            \"store_id\": randint(1, 10),\n            \"date\": datetime.today(),\n            \"sku\": randint(1, 50),\n        }\n        for _ in range(50)\n    ]\n\n    logger.info(\"Fetched %s sales data rows\", len(sales))\n\n    # Return the results of your task to have it stored\n    # and accessible on the web UI\n    # If you have other tasks, the output of a task is\n    # passed to the following one\n    return sales\n</code></pre>"},{"location":"create-a-pipeline/#create-a-pipeline","title":"Create a pipeline","text":"<p>A Pipeline contains a list of tasks and eventually a list of triggers, so in your <code>app.py</code> add this:</p> src/app.py<pre><code>register_pipeline(\n    id=\"sales_pipeline\",\n    description=\"Aggregate sales activity from all stores across the country\",\n    tasks = [fetch_raw_sales_data],\n    triggers = [\n        Trigger(\n            id=\"daily\",\n            name=\"Daily\",\n            description=\"Run the pipeline every day\",\n            schedule=IntervalTrigger(days=1),\n        ),\n    ],\n)\n</code></pre> <p>Finally add this at the bottom of your file to start the app:</p> src/app.py<pre><code>if __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(\"plombery:get_app\", reload=True, factory=True)\n</code></pre> <p>Now your <code>src/app.py</code> should look like this:</p> Click to see the full content of src/app.py src/app.py<pre><code>from datetime import datetime\nfrom random import randint\n\nfrom apscheduler.triggers.interval import IntervalTrigger\nfrom plombery import task, get_logger, Trigger, register_pipeline\n\n\n@task\nasync def fetch_raw_sales_data():\n    \"\"\"Fetch latest 50 sales of the day\"\"\"\n\n    # using Plombery logger your logs will be stored\n    # and accessible on the web UI\n    logger = get_logger()\n\n    logger.debug(\"Fetching sales data...\")\n\n    sales = [\n        {\n            \"price\": randint(1, 1000),\n            \"store_id\": randint(1, 10),\n            \"date\": datetime.today(),\n            \"sku\": randint(1, 50),\n        }\n        for _ in range(50)\n    ]\n\n    logger.info(\"Fetched %s sales data rows\", len(sales))\n\n    # Return the results of your task to have it stored\n    # and accessible on the web UI\n    return sales\n\n\nregister_pipeline(\n    id=\"sales_pipeline\",\n    description=\"Aggregate sales activity from all stores across the country\",\n    tasks=[fetch_raw_sales_data],\n    triggers=[\n        Trigger(\n            id=\"daily\",\n            name=\"Daily\",\n            description=\"Run the pipeline every day\",\n            schedule=IntervalTrigger(days=1),\n        ),\n    ],\n)\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(\"plombery:get_app\", reload=True, factory=True)\n</code></pre>"},{"location":"create-a-pipeline/#run-the-app","title":"Run the app","text":"<p>Plombery is based on FastAPI so you can run it as a normal FastAPI app via <code>uvicorn</code> (as in this example) or another ASGI web server.</p> <p>So install <code>uvicorn</code> and run the app:</p> <pre><code>pip install uvicorn\npython src/app.py\n</code></pre> <p>Now open the page http://localhost:8000 in your browser and enjoy!</p>"},{"location":"get-started/","title":"Install","text":""},{"location":"get-started/#prerequisites","title":"Prerequisites","text":"<p>To run Plombery you only need Python (v3.8 or later), if you don't have it installed yet, go to the official Python website, download it and install it.</p>"},{"location":"get-started/#installation","title":"Installation","text":"<p>It's a good practice to install dependencies specific to a project in a dedicated virtual environment for that project.</p> <p>Many code editors (IDE) provide their own way to create virtual environments, otherwise you can use directly the shell typing the following commands.</p> <p>Create a virtual enrivonment:</p> <pre><code># Run this in your project folder\npython -m venv .venv\n</code></pre> <p>Activate it:</p> <pre><code># on Mac/Linux\nsource .venv/bin/activate\n</code></pre> <pre><code># on Win\n.venv/Script/activate\n</code></pre> <p>Then install the library:</p> <pre><code>pip install plombery\n</code></pre> <p>Now you're ready to write your first pipeline!</p>"},{"location":"get-started/#try-on-github-codespaces","title":"\ud83c\udfae Try on GitHub Codespaces","text":"<p>If you don't want to setup the project locally and you just want to have a look at Plombery and how it works, then you should try GitHub Codespaces:</p> <p>Codespaces are development environments that run in the cloud so you can run a project without cloning it, installing deps etc, here's an how to:</p> <ul> <li>Go to the the lucafaggianelli/plombery GitHub page</li> <li>Click on the green Code button on the top right</li> <li>Choose the Codespaces tab</li> <li>Click on create new codespace from main or reuse an existing one</li> <li>A new page will open at <code>github.dev</code>, wait for the environment build</li> <li>Once your codespace is ready you'll see an interface similar to VSCode</li> <li>Some commands will be run in the terminal to build the frontend etc., wait for their completion</li> <li>If everything went well, Plombery home page will be open in a new browser tab</li> <li>Changes in the Python code will be immediately reflected in the web page, like if you were developing   on your laptop</li> </ul>"},{"location":"pipelines/","title":"Pipelines","text":"<p>A pipeline is a list of tasks that are executed sequentially.</p> <p>To declare a pipeline just call the function <code>register_pipeline</code>, the only 2 mandatory fields are <code>id</code> and <code>tasks</code>:</p> <pre><code>from plombery import register_pipeline, task\n\nclass InputParams(BaseModel):\n  some_value: int\n\n@task\ndef get_sales_data():\n  pass\n\nregister_pipeline(\n    # (required) the id identifies the pipeline univocally\n    id=\"sales_pipeline_2345\",\n    # (required) the list of tasks to execute\n    tasks=[get_sales_data],\n    # This pipeline is configurable via input parameters\n    params=InputParams,\n    # The name is optional, if absent it would be generated from the ID\n    name=\"Sales pipeline\",\n    description=\"\"\"This is a very useless pipeline\"\"\",\n    # Triggers with schedules\n    triggers=[\n        Trigger(\n            id=\"daily\",\n            name=\"Daily\",\n            description=\"Run the pipeline every day\",\n            # the input params value for this specific trigger\n            params=InputParams(some_value=2),\n            schedule=IntervalTrigger(\n                days=1,\n            ),\n        )\n    ],\n)\n</code></pre>"},{"location":"pipelines/#parameters","title":"Parameters","text":"<p>A pipeline is configurable if it declares some input parameters in the registration via the <code>params</code> argument:</p> <pre><code>register_pipeline(\n  # ...\n  params=InputParams\n)\n</code></pre> <p>The <code>InputParams</code> is a Pydantic Model:</p> <pre><code>class InputParams(BaseModel):\n  some_value: int\n</code></pre> <p>If the pipeline has input parameters, when you click the manual run button, the dialog will present a form to let you customize the input parameters:</p> <p> </p> Manual run with parameters <p>The input form in the dialog is created automatically thanks to the Pydantic's <code>BaseModel</code> that you declared in the pipeline.</p> <p>Parameters are configurable also when you run a pipeline via the HTTP trigger, just pass the parameters as JSON body in the HTTP request.</p>"},{"location":"tasks/","title":"Tasks","text":"<p>A task is just a regular Python function decorated with the <code>task</code> decorator, the functions can be also <code>async</code>, Plombery will take care of everything:</p> <pre><code>@task\ndef sync_task():\n  pass\n\n@task\nasync def async_task():\n  pass\n</code></pre> <p>Then pass the function names to the <code>register_pipeline</code> function:</p> <pre><code>register_pipeline(\n  tasks=[sync_task, async_task]\n)\n</code></pre>"},{"location":"tasks/#input-parameters","title":"Input parameters","text":"<p>If the pipeline declares input parameters:</p> <pre><code>class InputParams(BaseModel):\n  some_value: int\n\nregister_pipeline(\n  # ...\n  params=InputParams\n)\n</code></pre> <p>then the task function will receive those input parameters via the <code>params</code> argument:</p> <pre><code>@task\nasync def my_task(params: InputParams):\n  result = params.some_value + 8\n</code></pre>"},{"location":"tasks/#output-data","title":"Output data","text":"<p>In Plombery, pipelines execute their tasks sequentially and the return value of a task is considered its output data that is passed to the next ones as positional arguments:</p> <pre><code>@task\ndef task_1():\n  return 1\n\n@task\ndef task_2(from_1):\n  # from_1 = 1\n  return from_1 + 1\n\n@task\ndef task_3(from_1, from_2):\n  # from_1 = 1\n  # from_2 = 2\n  return from_1 + from_2\n</code></pre>"},{"location":"tasks/#logging","title":"Logging","text":"<p>Plombery collects automatically pipelines logs and shows them on the UI:</p> <p> </p> Pipeline run logs <p>To use this feature, you need to use a plombery's logger simply calling the <code>get_logger</code> function:</p> <pre><code>from plombery import get_logger\n\n@task\ndef my_task():\n  logger = get_logger()\n  logger.debug(\"Hey greetings!\")\n</code></pre> <p>Warning</p> <p><code>get_logger</code> is a special function that only works inside tasks functions: don't call it outside of those functions as it won't work! <pre><code># \u274c Don't do this\nlogger = get_logger()\ndef my_task():\n  logger.debug(\"Hey greetings!\")\n</code></pre></p>"},{"location":"triggers/","title":"Triggers","text":"<p>A trigger is the entrypoint to run a pipeline, it can be a schedule or an external event and eventually has input parameters associated to it, if the pipeline has declared any parameter.</p>"},{"location":"triggers/#pipeline-trigger","title":"Pipeline trigger","text":"<p>When you register a pipeline, you're not obliged to add triggers. By default the system will add a pipeline trigger to let you run the pipeline without any additional effort.</p> <p>The pipeline trigger results in a button on the web UI and an HTTP endpoint which you can use to run the pipeline.</p>"},{"location":"triggers/#http-trigger","title":"HTTP trigger","text":"<p>The HTTP trigger allows you to run the pipeline via an HTTP <code>POST</code> call. If you open a pipeline page you'll find the trigger URL:</p> <p> </p> URL to run the pipeline programmatically via an HTTP POST request"},{"location":"triggers/#manual-trigger","title":"Manual trigger","text":"<p>The web UI features a manual run button (which is based on the HTTP trigger). You will find this button in the home page and in the pipeline page.</p>"},{"location":"triggers/#with-parameters","title":"With parameters","text":"<p>If the pipeline has input parameters, when you click the manual run button, the dialog will present a form to let you customize the input parameters:</p> <p> </p> Manual run with parameters <p>The input form in the dialog is created automatically thanks to the Pydantic's <code>BaseModel</code> that you declared in the pipeline.</p> <p>Parameters are configurable also when you run a pipeline via the HTTP trigger, just pass the parameters as JSON body in the HTTP request.</p>"},{"location":"triggers/#schedules","title":"Schedules","text":"<p>Scheduling a pipeline is probably the first thing you're looking for when using Plombery. It's very easy, just add 1 or more triggers when registering a pipeline, (the <code>triggers</code> argument must be a list even if you only have 1 trigger). The actual schedule is defined via the <code>schedule</code> argument:</p> <pre><code>from apscheduler.triggers.interval import IntervalTrigger\nfrom plombery import register_pipeline, Trigger\n\nregister_pipeline(\n    id=\"sales_pipeline\",\n    tasks=[get_sales_data],\n    triggers=[\n        Trigger(\n            id=\"daily\",\n            description=\"Run the pipeline every day\",\n            schedule=IntervalTrigger(\n                days=1,\n            ),\n        ),\n    ],\n)\n</code></pre> <p><code>schedule</code> accepts any APS trigger, at the moment the following schedules/APS triggers are available:</p> <ul> <li><code>CronTrigger</code></li> <li><code>DateTrigger</code></li> <li><code>IntervalTrigger</code></li> <li><code>Combining</code></li> </ul>"},{"location":"triggers/#triggers-with-parameters","title":"Triggers with parameters","text":"<p>Adding triggers to a pipeline is not useful only for scheduling purposes, but also for definign alternative entrypoints to run a pipeline with custom parameters.</p> <p>If a pipeline has input parameters, then its triggers can have custom parameters:</p> <pre><code>from pydantic import BaseModel\n\nclass InputParams(BaseModel):\n    past_days: int\n    convert_currency: bool = False\n\ndef get_sales_data(params: InputParams):\n    print(params.past_days)\n\nregister_pipeline(\n    id=\"sales_pipeline\",\n    tasks=[get_sales_data],\n    params=InputParams,\n    triggers=[\n        Trigger(\n            id=\"daily\",\n            description=\"Get last 5 days of sales data in USD dollars\",\n            schedule=IntervalTrigger(\n                days=1,\n            ),\n            params={\n                \"past_days\": 5,\n                \"convert_currency\": True,\n            },\n        ),\n    ],\n)\n</code></pre> <p>In the triggers page you'll find info about the trigger, including its URL and its parameters.</p> <p> </p> Trigger info, in the trigger page <p>Be aware that you cannot customize the input parameters when running a trigger (manually or via HTTP), so be sure to provide a valid object in the trigger's <code>params</code> argument.</p> <p></p> <p>Indeed when you click the Run trigger button, there will be no dialog to confirm the run or customize the parameters.</p> <p>Why?</p> <p>This is a design choice. A trigger is by definition a well-defined entrypoint that could run without human intervention, that is via a schedule, and to have consistent runs of the same trigger either via a schedule or via a UI button, the trigger must be immutable.</p> <p>For example, based on the previous code, you can't declare the trigger like this, because <code>past_days</code> is missing but it's mandatory in <code>InputParams</code>:</p> <p><pre><code>Trigger(\n    # ...\n    params={\n        \"convert_currency\": True,\n    }\n)\n</code></pre> Though you can omit optional values, so this would be valid:</p> <pre><code>Trigger(\n    # ...\n    params={\n        \"past_days\": 3,\n    }\n)\n</code></pre> <p>If you have a complex input parameters model, it's very useful to declare several triggers, without a schedule, just to have clear entrypoints for a user that wants to run the pipeline manually and doesn't know much about the code.</p>"},{"location":"configuration/","title":"Overview","text":"<p>Plombery is configurable via environmental variables, a YAML file or even better via a combination of the 2.</p> <p>Why a hybrid configuration?</p> <p>An entire configuration can be quite large so storing it as environmental variables can be quite hard to maintain, moreover some parts of the configuration should be stored together with the code as they are part of the system and some parts of it are secret so you need env vars</p> <p>Create a configuration file in the root of your project named <code>plombery.config.yaml</code> (or <code>plombery.config.yml</code> if you prefer) and set the values you need, you should commit this file to the git repo:</p> plombery.config.yaml<pre><code>frontend_url: https://pipelines.example.com\n\nauth:\n  client_id: $GOOGLE_CLIENT_ID\n  client_secret: $GOOGLE_CLIENT_SECRET\n  server_metadata_url: https://accounts.google.com/.well-known/openid-configuration\n\nnotifications:\n  - pipeline_status:\n      - failed\n    channels:\n      - $GMAIL_ACCOUNT\n      - $MSTEAMS_WEBHOOK\n</code></pre> <p>Now define the secrets as environmental variables in a <code>.env</code> file, in your shell or in your hosting environment.</p> <p>Tip</p> <p>By default, Plombery will load any <code>.env</code> found in your project root.</p> <p>Warning</p> <p>You shouldn't commit the <code>.env</code> file as it contains secrets!</p> .env<pre><code># Auth\nGOOGLE_CLIENT_ID=\"ABC123\"\nGOOGLE_CLIENT_SECRET=\"DEF456\"\n\n# Notifications\nGMAIL_ACCOUNT=mailto://myuser:mypass@gmail.com\nMSTEAMS_WEBHOOK=msteams://TokenA/TokenB/TokenC/\n</code></pre>"},{"location":"configuration/system/","title":"System","text":"<p>Tip</p> <p>If you're running Plombery locally, in most cases you don't need to change these settings</p>"},{"location":"configuration/system/#allowed_origins","title":"<code>allowed_origins</code>","text":"<p>Change it if running in production.</p> <p>It allows to configure the CORS header <code>Access-Control-Allow-Origin</code>, by default it's value is <code>*</code> so it allows all origins.</p>"},{"location":"configuration/system/#data_path","title":"<code>data_path</code>","text":"<p>The absolute path to the data directory where logs and output data is stored.</p> <p>By default is set to the current working directory.</p>"},{"location":"configuration/system/#database_url","title":"<code>database_url</code>","text":"<p>The Sqlite DB URI, by default <code>sqlite:///./plombery.db</code></p>"},{"location":"configuration/system/#frontend_url","title":"<code>frontend_url</code>","text":"<p>The URL of the frontend, by default is the same as the backend, change it if the frontend is served at a different URL, for example during the frontend development.</p>"},{"location":"configuration/auth/generic-oauth/","title":"Generic OAuth","text":"<p>Plombery has a buil-in and ready-to-use authentication system based on OAuth providers, so you can use your corporate auth system or Google, Github, etc. as long as they're compatible with OAuth.</p> <p>To enable the auth system you just need to configure it via the <code>yml</code> config file.</p> <p>Pre-configured providers</p> <p>This page shows how to configure a generic OAuth provider, though there are some providers already preconfigured by Plombery, check if your provider is available in the Authentication section of the docs.</p> <p>Good to know</p> <p>The auth system is based on Authlib</p>"},{"location":"configuration/auth/generic-oauth/#authsettings","title":"<code>AuthSettings</code>","text":"<p>Options available</p>"},{"location":"configuration/auth/generic-oauth/#client_id","title":"<code>client_id</code>","text":"<p>An OAuth app client ID</p>"},{"location":"configuration/auth/generic-oauth/#client_secret","title":"<code>client_secret</code>","text":"<p>An OAuth app client secret</p>"},{"location":"configuration/auth/generic-oauth/#server_metadata_url","title":"<code>server_metadata_url</code>","text":"<p>This a special URL that contains information about the OAuth provider specific endpoints. If your provider doesn't have this URL or you don't know it, you need to fill up the values for the other URLs: <code>access_token_url</code>, <code>authorize_url</code> and <code>jwks_uri</code>.</p> <p>For example, for Google the URL is:</p> <pre><code>https://accounts.google.com/.well-known/openid-configuration\n</code></pre>"},{"location":"configuration/auth/generic-oauth/#access_token_url","title":"<code>access_token_url</code>","text":""},{"location":"configuration/auth/generic-oauth/#authorize_url","title":"<code>authorize_url</code>","text":""},{"location":"configuration/auth/generic-oauth/#jwks_uri","title":"<code>jwks_uri</code>","text":""},{"location":"configuration/auth/generic-oauth/#client_kwargs","title":"<code>client_kwargs</code>","text":"<p>Additional values to pass to the OAuth client during the auth process, for example the scope:</p> <pre><code>auth:\n  client_kwargs:\n    scope: openid email profile\n</code></pre>"},{"location":"configuration/auth/generic-oauth/#secret_key","title":"<code>secret_key</code>","text":"<p>Secret key used in the backend middleware, this has a dummy default value, but in production you should define a decent value.</p>"},{"location":"configuration/auth/google/","title":"Google","text":"<p>Google authentication is supported out-of-the-box, this is the configuration needed in <code>plombery.config.yaml</code>:</p> plombery.config.yaml<pre><code>auth:\n  provider: google\n  client_id: $GOOGLE_CLIENT_ID\n  client_secret: $GOOGLE_CLIENT_SECRET\n</code></pre> <p>The values starting with <code>$</code> are replaced with environmental variables, in this way secret values are decoupled from the configuration file that is normally versioned in git, you can modify the name of those variables as long as they match the names used in the <code>plombery.config.yaml</code>:</p> .env<pre><code>GOOGLE_TENANT_ID=\"\"\nGOOGLE_CLIENT_ID=\"\"\n</code></pre>"},{"location":"configuration/auth/microsoft/","title":"Microsoft","text":"<p>Microsoft authentication is supported out-of-the-box, this is the configuration needed in <code>plombery.config.yaml</code>:</p> plombery.config.yaml<pre><code>auth:\n  provider: microsoft\n  client_id: $MICROSOFT_CLIENT_ID\n  client_secret: $MICROSOFT_CLIENT_SECRET\n  microsoft_tenant_id: $MICROSOFT_TENANT_ID\n</code></pre> <p>The values starting with <code>$</code> are replaced with environmental variables, in this way secret values are decoupled from the configuration file that is normally versioned in git, you can modify the name of those variables as long as they match the names used in the <code>plombery.config.yaml</code>:</p> .env<pre><code>MICROSOFT_TENANT_ID=\"\"\nMICROSOFT_CLIENT_ID=\"\"\nMICROSOFT_CLIENT_SECRET=\"\"\n</code></pre>"},{"location":"configuration/auth/microsoft/#tenant-id","title":"Tenant ID","text":"<p>The <code>microsoft_tenant_id</code> config is optional and depends on how you registered your application in Azure.</p> <p>If you configured your Azure Application Registration to allow users only from your tenant, then you need to supply this option, otherwise this option must be omitted and the <code>common</code> tenant is used in the OAuth endpoints.</p>"},{"location":"configuration/notifications/","title":"Notifications","text":"<p>Plombery can send notifications after a pipeline has run based on the status of the run itself (success, failure, etc.).</p> <p>The notifications configuration can be defined in the YAML file as a list of <code>NotificationRule</code>s:</p> plombery.config.yaml<pre><code>notifications:\n  # Send notifications only if the pipelines failed\n  - pipeline_status:\n      - failed\n    channels:\n      # Send them to my gmail address (from my address itself)\n      # Better to use an env var here\n      - mailto://myuser:mypass@gmail.com\n  # Send notifications only if the pipelines succeeded or was cancelled\n  - pipeline_status:\n      - completed\n      - cancelled\n    channels:\n      # Send them to a MS Teams channel\n      # Better to use an env var here\n      - msteams://mychanneltoken\n</code></pre>"},{"location":"configuration/notifications/#notificationrule","title":"<code>NotificationRule</code>","text":"<p>A notification rule defines when to send notifications and to whom.</p>"},{"location":"configuration/notifications/#pipeline_status","title":"<code>pipeline_status</code>","text":"<p>A list of 1 or more pipeline run status among:</p> <ul> <li><code>completed</code></li> <li><code>failed</code></li> <li><code>cancelled</code></li> </ul>"},{"location":"configuration/notifications/#channels","title":"<code>channels</code>","text":"<p>A list of 1 or more recipients where to send the notifications.</p> <p>A channel is an Apprise URI string that defines an email address or a MS Teams channel, for example:</p> <ul> <li>Email mailto://myuser:mypass@gmail.com</li> <li>MS Teams msteams://TokenA/TokenB/TokenC/</li> <li>AWS SES ses://user@domain/AccessKeyID/AccessSecretKey/RegionName/email1/</li> </ul> <p>Behind the scene Plombery uses Apprise, a library to send notifications to many notification providers, so check their docs for a full list of the available channels.</p>"},{"location":"recipes/certificates-expiration/","title":"SSL Certificate Check","text":"<p>Example</p> <p>The full source code for this example is available here</p> <p>Managing SSL certificate expiration dates manually can be a challenging and error-prone task, especially in environments with a large number of hostnames to check.</p> <p>Failing to renew certificates on time can result in unexpected outages and security breaches. To address this issue, you can use Plombery to automate the monitoring of SSL certificate expiration dates and receive notifications when a certificate is due to expire.</p> <p> </p> The SSL check pipeline <p> </p> The run page with logs <p>You can also run an SSL check on the fly via the manual trigger:</p> <p> </p> Manual run"},{"location":"recipes/certificates-expiration/#how-to","title":"How to","text":"<p>Define a list of hostnames you want to check and create 1 trigger for each of them so once a trigger fails you know which hostname is concerned:</p> <pre><code>hostnames = [\n    \"google.com\",\n    \"velvetlab.tech\",\n]\n\nregister_pipeline(\n    id=\"check_ssl_certificate\",\n    name=\"Check SSL certificate\",\n    description=\"\"\"Check if the SSL certificate of a website has expired\"\"\",\n    tasks=[check_certificate_expiration],\n    triggers=[\n        # Create 1 trigger per each host to check\n        Trigger(\n            id=f\"check-{host}\",\n            name=host,\n            description=\"Run the pipeline every week\",\n            params=InputParams(hostname=host),\n            schedule=IntervalTrigger(\n                weeks=1,\n            ),\n        )\n        for host in hostnames\n    ],\n    params=InputParams,\n)\n</code></pre> <p>The pipeline in this example has only 1 task but you could add additional checks on the SSL certificate as additional tasks:</p> <pre><code>@task\nasync def check_certificate_expiration(params: InputParams):\n    logger = get_logger()\n    now = datetime.utcnow()\n\n    info = get_certificate_info(params.hostname)\n    expiration: datetime = info.get(\"notAfter\")\n\n    if expiration &lt;= now:\n        raise Exception(f\"The certificate expired on {expiration}\")\n\n    expires_in = expiration - now\n\n    if expires_in.days &lt; EXPIRATION_WARNING_THRESHOLD:\n        raise Exception(f\"Attention, the certificate expires in {expires_in.days} days\")\n\n    logger.info(\n        f\"All good, the certificate expires in {expires_in.days} days on the {expiration}\"\n    )\n</code></pre>"}]}